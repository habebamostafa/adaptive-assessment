import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
import json
from datetime import datetime

# Import our enhanced modules
from core.environment import AdaptiveAssessmentEnv
from core.agent import RLAssessmentAgent, AdaptiveStrategy, MultiAgentEnsemble
from data.questions import get_adaptive_question, _question_manager, get_question_statistics

# Page configuration
st.set_page_config(
    page_title="ğŸ¯ Adaptive Assessment System",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 1rem;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #667eea;
    }
    
    .question-card {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border: 1px solid #e9ecef;
        margin: 1rem 0;
    }
    
    .success-message {
        background: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid #c3e6cb;
    }
    
    .warning-message {
        background: #fff3cd;
        color: #856404;
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid #ffeaa7;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
def initialize_session_state():
    """Initialize all session state variables"""
    defaults = {
        "initialized": False,
        "answer_confirmed": False,
        "show_results": False,
        "current_question": None,
        "selected_answer": None,
        "assessment_complete": False,
        "show_analytics": False,
        "agent_type": "main",
        "adaptation_strategy": "rl_based"
    }
    
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

# Sidebar configuration
def render_sidebar():
    """Render the sidebar with configuration options"""
    st.sidebar.title("âš™ï¸ Assessment Settings")
    
    # Track selection
    available_tracks = _question_manager.generator.get_available_tracks()
    track_descriptions = {
        "web": "ğŸŒ Web Development",
        "ai": "ğŸ¤– Artificial Intelligence",
        "cyber": "ğŸ” Cybersecurity",
        "data": "ğŸ“Š Data Science",
        "mobile": "ğŸ“± Mobile Development",
        "devops": "â˜ï¸ DevOps & Cloud"
    }
    
    selected_track = st.sidebar.selectbox(
        "Ø§Ø®ØªØ± Ø§Ù„ØªØ®ØµØµ:",
        options=available_tracks,
        format_func=lambda x: track_descriptions.get(x, x.title()),
        key="track_selector"
    )
    
    # Agent configuration
    st.sidebar.subheader("ğŸ¤– Agent Settings")
    
    agent_type = st.sidebar.selectbox(
        "Ù†ÙˆØ¹ Ø§Ù„ÙˆÙƒÙŠÙ„:",
        options=["main", "conservative", "aggressive", "ensemble"],
        format_func=lambda x: {
            "main": "ğŸ¯ Ù…ØªÙˆØ§Ø²Ù†",
            "conservative": "ğŸ›¡ï¸ Ù…Ø­Ø§ÙØ¸", 
            "aggressive": "âš¡ Ø¬Ø±ÙŠØ¡",
            "ensemble": "ğŸ­ Ù…ØªØ¹Ø¯Ø¯"
        }.get(x, x)
    )
    
    adaptation_strategy = st.sidebar.selectbox(
        "Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªÙƒÙŠÙ:",
        options=["rl_based", "conservative", "aggressive", "ability_based"],
        format_func=lambda x: {
            "rl_based": "ğŸ§  ØªØ¹Ù„Ù… Ù…Ø¹Ø²Ø²",
            "conservative": "ğŸŒ Ù…Ø­Ø§ÙØ¸Ø©",
            "aggressive": "ğŸš€ Ø³Ø±ÙŠØ¹Ø©", 
            "ability_based": "ğŸ“ˆ Ø­Ø³Ø¨ Ø§Ù„Ù‚Ø¯Ø±Ø©"
        }.get(x, x)
    )
    
    # Assessment parameters
    st.sidebar.subheader("ğŸ“‹ Assessment Parameters")
    
    max_questions = st.sidebar.slider(
        "Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø£Ø³Ø¦Ù„Ø©:",
        min_value=5,
        max_value=20,
        value=10,
        help="Maximum number of questions in the assessment"
    )
    
    confidence_threshold = st.sidebar.slider(
        "Ø¹ØªØ¨Ø© Ø§Ù„Ø«Ù‚Ø©:",
        min_value=0.5,
        max_value=0.95,
        value=0.8,
        step=0.05,
        help="Confidence threshold for early termination"
    )
    
    return selected_track, agent_type, adaptation_strategy, max_questions, confidence_threshold

# Analytics and visualizations
def render_analytics():
    """Render analytics dashboard"""
    if not st.session_state.get("env") or not st.session_state.env.question_history:
        st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ø±Ø¶. ÙŠØ±Ø¬Ù‰ Ø¥ÙƒÙ…Ø§Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø£ÙˆÙ„Ø§Ù‹.")
        return
    
    env = st.session_state.env
    agent = st.session_state.agent
    
    st.header("ğŸ“Š Analytics Dashboard")
    
    # Performance metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_questions = len(env.question_history)
        st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©", total_questions)
    
    with col2:
        correct_answers = sum(1 for q in env.question_history if q['is_correct'])
        accuracy = correct_answers / total_questions if total_questions > 0 else 0
        st.metric("Ø§Ù„Ø¯Ù‚Ø©", f"{accuracy:.1%}")
    
    with col3:
        st.metric("Ø§Ù„Ù‚Ø¯Ø±Ø© Ø§Ù„Ù…Ù‚Ø¯Ø±Ø©", f"{env.student_ability:.1%}")
    
    with col4:
        st.metric("Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©", f"{env.confidence_score:.1%}")
    
    # Progress visualization
    if len(env.performance_history) > 1:
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=("Ability Progression", "Performance by Level", 
                          "Confidence Over Time", "Question Difficulty"),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"type": "domain"}]]
        )
        
        # Ability progression
        questions = [p['question_number'] for p in env.performance_history]
        abilities = [p['ability'] for p in env.performance_history]
        
        fig.add_trace(
            go.Scatter(x=questions, y=abilities, mode='lines+markers',
                      name='Student Ability', line=dict(color='blue')),
            row=1, col=1
        )
        
        # Performance by level
        level_data = {}
        for q in env.question_history:
            level = q['level']
            if level not in level_data:
                level_data[level] = {'correct': 0, 'total': 0}
            level_data[level]['total'] += 1
            if q['is_correct']:
                level_data[level]['correct'] += 1
        
        levels = list(level_data.keys())
        accuracies = [level_data[l]['correct'] / level_data[l]['total'] for l in levels]
        
        fig.add_trace(
            go.Bar(x=[f"Level {l}" for l in levels], y=accuracies,
                   name='Accuracy by Level', marker_color='green'),
            row=1, col=2
        )
        
        # Confidence over time
        confidences = [p['confidence'] for p in env.performance_history]
        
        fig.add_trace(
            go.Scatter(x=questions, y=confidences, mode='lines+markers',
                      name='Confidence', line=dict(color='orange')),
            row=2, col=1
        )
        
        # Question difficulty distribution
        difficulty_counts = [0, 0, 0]  # Easy, Medium, Hard
        for q in env.question_history:
            difficulty_counts[q['level'] - 1] += 1
        
        fig.add_trace(
            go.Pie(labels=['Easy', 'Medium', 'Hard'], values=difficulty_counts,
                   name='Question Distribution'),
            row=2, col=2
        )
        
        fig.update_layout(height=600, showlegend=True)
        st.plotly_chart(fig, use_container_width=True)
    
    # Agent performance
    st.subheader("ğŸ¤– Agent Performance")
    agent_metrics = agent.get_performance_metrics()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.json(agent_metrics)
    
    with col2:
        # Q-table visualization
        q_summary = agent.get_q_table_summary()
        if q_summary:
            q_df = pd.DataFrame(q_summary).T
            st.subheader("Q-Table Values")
            st.dataframe(q_df)
    
    # Detailed question history
    st.subheader("ğŸ“ Question History")
    
    history_data = []
    for i, q in enumerate(env.question_history, 1):
        history_data.append({
            'Question #': i,
            'Level': q['level'],
            'Question': q['question']['text'][:50] + "...",
            'Your Answer': q['answer'],
            'Correct Answer': q['question']['correct_answer'],
            'Result': "âœ…" if q['is_correct'] else "âŒ",
            'Ability After': f"{q['student_ability_after']:.2%}"
        })
    
    if history_data:
        df = pd.DataFrame(history_data)
        st.dataframe(df, use_container_width=True)

# Question rendering
def render_question():
    """Render the current question"""
    if not st.session_state.current_question:
        st.error("âŒ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„.")
        return
    
    q = st.session_state.current_question
    env = st.session_state.env
    
    # Question header
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        st.markdown(f"### Ø§Ù„Ø³Ø¤Ø§Ù„ Ø±Ù‚Ù… {env.total_questions_asked}")
    
    with col2:
        level_emoji = {1: "ğŸŸ¢", 2: "ğŸŸ¡", 3: "ğŸ”´"}
        level_name = {1: "Ø³Ù‡Ù„", 2: "Ù…ØªÙˆØ³Ø·", 3: "ØµØ¹Ø¨"}
        st.markdown(f"**Ø§Ù„Ù…Ø³ØªÙˆÙ‰:** {level_emoji.get(env.current_level, 'âšª')} {level_name.get(env.current_level, 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
    
    with col3:
        st.markdown(f"**Ø§Ù„Ù‚Ø¯Ø±Ø©:** {env.student_ability:.1%}")
    
    # Question content
    st.markdown(f"""
    <div class="question-card">
        <h4>{q['text']}</h4>
    </div>
    """, unsafe_allow_html=True)
    
    # Answer options
    if "selected_answer" not in st.session_state:
        st.session_state.selected_answer = None
    
    st.session_state.selected_answer = st.radio(
        "Ø§Ø®ØªØ± Ø¥Ø¬Ø§Ø¨Ø©:",
        q["options"],
        key=f"question_{env.total_questions_asked}",
        index=None
    )
    
    # Action buttons
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        confirm_disabled = (st.session_state.selected_answer is None or 
                          st.session_state.answer_confirmed)
        
        if st.button("âœ… ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", disabled=confirm_disabled):
            st.session_state.answer_confirmed = True
            st.rerun()
    
    with col2:
        if st.session_state.answer_confirmed:
            if st.button("â¡ï¸ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ"):
                process_answer()
    
    with col3:
        if st.session_state.answer_confirmed:
            # Show correct answer
            is_correct = q['correct_answer'] == st.session_state.selected_answer
            if is_correct:
                st.success("ğŸ‰ Ø¥Ø¬Ø§Ø¨Ø© ØµØ­ÙŠØ­Ø©!")
            else:
                st.error(f"âŒ Ø¥Ø¬Ø§Ø¨Ø© Ø®Ø§Ø·Ø¦Ø©. Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: {q['correct_answer']}")
            
            # Show explanation if available
            if 'explanation' in q:
                st.info(f"ğŸ’¡ {q['explanation']}")

def process_answer():
    """Process the student's answer and update the system"""
    env = st.session_state.env
    agent = st.session_state.agent
    q = st.session_state.current_question
    answer = st.session_state.selected_answer
    
    # Get current state
    current_state = agent.get_state()
    
    # Submit answer to environment
    reward, done = env.submit_answer(q, answer)
    
    # Get next state
    next_state = agent.get_state()
    
    # Choose action based on strategy
    if st.session_state.adaptation_strategy == "rl_based":
        action = agent.choose_action(next_state)
    elif st.session_state.adaptation_strategy == "conservative":
        action = AdaptiveStrategy.conservative_strategy(next_state)
    elif st.session_state.adaptation_strategy == "aggressive":
        action = AdaptiveStrategy.aggressive_strategy(next_state)
    elif st.session_state.adaptation_strategy == "ability_based":
        action = AdaptiveStrategy.ability_based_strategy(next_state)
    else:
        action = "auto"
    
    # Update Q-table if using RL agent
    if st.session_state.adaptation_strategy == "rl_based":
        agent.update_q_table(current_state, action, reward, next_state)
    
    # Adjust difficulty
    agent.adjust_difficulty(action)
    
    # Reset for next question
    st.session_state.answer_confirmed = False
    st.session_state.selected_answer = None
    
    if done:
        st.session_state.assessment_complete = True
        st.session_state.show_results = True
    else:
        # Get next question
        next_question = env.get_question()
        if next_question:
            st.session_state.current_question = next_question
        else:
            st.session_state.assessment_complete = True
            st.session_state.show_results = True
    
    st.rerun()

def render_results():
    """Render the final results and summary"""
    env = st.session_state.env
    agent = st.session_state.agent
    
    st.markdown("""
    <div class="success-message">
        <h2 style="margin: 0;">ğŸ‰ ØªÙ… Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!</h2>
        <p style="margin: 0.5rem 0 0 0;">Ø¥Ù„ÙŠÙƒ Ù…Ù„Ø®Øµ Ø£Ø¯Ø§Ø¦Ùƒ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª:</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Get assessment summary
    summary = env.get_assessment_summary()
    
    # Main metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©",
            f"{summary['correct_answers']}/{summary['total_questions']}",
            f"{summary['final_score']:.1%}"
        )
    
    with col2:
        st.metric(
            "Ø§Ù„Ù‚Ø¯Ø±Ø© Ø§Ù„Ù…Ù‚Ø¯Ø±Ø©",
            f"{summary['final_ability']:.1%}",
            help="ØªÙ‚Ø¯ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù‚Ø¯Ø±Ø© Ø§Ù„Ø·Ø§Ù„Ø¨"
        )
    
    with col3:
        st.metric(
            "Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©",
            f"{summary['confidence_score']:.1%}",
            help="Ù…Ø¯Ù‰ Ø«Ù‚Ø© Ø§Ù„Ù†Ø¸Ø§Ù… ÙÙŠ Ø§Ù„ØªÙ‚Ø¯ÙŠØ±"
        )
    
    with col4:
        recommended_level = summary['recommended_level']
        level_names = {1: "Ù…Ø¨ØªØ¯Ø¦", 2: "Ù…ØªÙˆØ³Ø·", 3: "Ù…ØªÙ‚Ø¯Ù…"}
        st.metric(
            "Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡",
            level_names.get(recommended_level, "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
        )
    
    # Performance by level
    st.subheader("ğŸ“Š Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³ØªÙˆÙ‰")
    
    if summary['level_performance']:
        level_data = []
        for level, perf in summary['level_performance'].items():
            level_data.append({
                'Ø§Ù„Ù…Ø³ØªÙˆÙ‰': f"Level {level}",
                'Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©': perf['questions'],
                'Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©': perf['correct'],
                'Ù†Ø³Ø¨Ø© Ø§Ù„Ø¯Ù‚Ø©': f"{perf['accuracy']:.1%}"
            })
        
        df = pd.DataFrame(level_data)
        st.dataframe(df, use_container_width=True)
        
        # Visualization
        fig = px.bar(
            df, 
            x='Ø§Ù„Ù…Ø³ØªÙˆÙ‰', 
            y='Ù†Ø³Ø¨Ø© Ø§Ù„Ø¯Ù‚Ø©',
            title="Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø³Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø©",
            color='Ù†Ø³Ø¨Ø© Ø§Ù„Ø¯Ù‚Ø©',
            color_continuous_scale='RdYlGn'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # Detailed recommendations
    st.subheader("ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª ÙˆØ§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©")
    
    ability = summary['final_ability']
    
    if ability < 0.3:
        recommendations = [
            "ğŸ” Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„",
            "ğŸ“š Ø§Ø¯Ø±Ø³ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ù„Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†",
            "ğŸ¤ Ø§Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ù† Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„",
            "â° Ø®Ø° ÙˆÙ‚ØªÙƒ Ø§Ù„ÙƒØ§ÙÙŠ Ù„ÙÙ‡Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ§Øª"
        ]
        st.info("ğŸ’ª Ø£Ù†Øª ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø±Ø­Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù…. Ù„Ø§ ØªÙŠØ£Ø³!")
        
    elif ability < 0.7:
        recommendations = [
            "ğŸ“ˆ Ø£Ù†Øª ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­ØŒ Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªØ·ÙˆÙŠØ±",
            "ğŸ¯ Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚ÙŠØ©",
            "ğŸ“‹ Ø­Ù„ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ† ÙˆØ§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©",
            "ğŸ”„ Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙŠ ÙˆØ§Ø¬Ù‡Øª ØµØ¹ÙˆØ¨Ø© ÙÙŠÙ‡Ø§"
        ]
        st.success("ğŸ‘ Ø£Ø¯Ø§Ø¡ Ø¬ÙŠØ¯! ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­Ø³Ù† Ø£ÙƒØ«Ø±")
        
    else:
        recommendations = [
            "ğŸ† Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø²! Ø£Ù†Øª ØªØªÙ‚Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„",
            "ğŸš€ ÙÙƒØ± ÙÙŠ ØªØ¹Ù„Ù… Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù…ØªÙ‚Ø¯Ù…Ø© Ø£ÙƒØ«Ø±",
            "ğŸ‘¨â€ğŸ« ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù…",
            "ğŸ”¬ Ø§Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ù…Ø´Ø§Ø±ÙŠØ¹ ØªØ­Ø¯ÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„"
        ]
        st.balloons()
        st.success("ğŸŒŸ Ø£Ø¯Ø§Ø¡ Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠ! ØªÙ‡Ø§Ù†ÙŠÙ†Ø§!")
    
    for rec in recommendations:
        st.write(rec)
    
    # Learning path suggestions
    st.subheader("ğŸ›£ï¸ Ù…Ø³Ø§Ø± Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ù‚ØªØ±Ø­")
    
    track = env.track
    track_resources = {
        "web": {
            "beginner": ["HTML/CSS Ø£Ø³Ø§Ø³ÙŠØ§Øª", "JavaScript Ù„Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†", "Ù…Ø´Ø§Ø±ÙŠØ¹ Ø¨Ø³ÙŠØ·Ø©"],
            "intermediate": ["React Ø£Ùˆ Vue.js", "Node.js", "Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"],
            "advanced": ["Ù…ÙØ§Ù‡ÙŠÙ… Ù…ØªÙ‚Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ÙØ±ÙŠÙ…ÙˆÙˆØ±ÙƒØ§Øª", "DevOps", "Ø§Ù„Ø£Ù…Ø§Ù† ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª"]
        },
        "ai": {
            "beginner": ["Ø£Ø³Ø§Ø³ÙŠØ§Øª Python", "Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Pandas Ùˆ NumPy"],
            "intermediate": ["Machine Learning", "TensorFlow Ø£Ùˆ PyTorch", "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"],
            "advanced": ["Deep Learning", "NLP", "Computer Vision"]
        },
        "cyber": {
            "beginner": ["Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†", "Ø§Ù„Ø´Ø¨ÙƒØ§Øª", "Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ´ØºÙŠÙ„"],
            "intermediate": ["Penetration Testing", "Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø­ÙˆØ§Ø¯Ø«", "Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†"],
            "advanced": ["Malware Analysis", "Red Team Operations", "Security Architecture"]
        },
        "data": {
            "beginner": ["Ø¥Ø­ØµØ§Ø¡ Ø£Ø³Ø§Ø³ÙŠ", "Python Ø£Ùˆ R", "Excel Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"],
            "intermediate": ["ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", "ØªØµÙˆÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", "SQL"],
            "advanced": ["Machine Learning", "Big Data", "Data Engineering"]
        },
        "mobile": {
            "beginner": ["Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©", "UI/UX Design", "Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª"],
            "intermediate": ["Native Development", "Cross-platform", "Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©"],
            "advanced": ["Performance Optimization", "CI/CD Ù„Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª", "Architecture Patterns"]
        },
        "devops": {
            "beginner": ["Linux Ø£Ø³Ø§Ø³ÙŠØ§Øª", "Git", "Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø®ÙˆØ§Ø¯Ù…"],
            "intermediate": ["Docker", "CI/CD", "Cloud Platforms"],
            "advanced": ["Kubernetes", "Infrastructure as Code", "Monitoring"]
        }
    }
    
    if recommended_level == 1:
        level_key = "beginner"
    elif recommended_level == 2:
        level_key = "intermediate"
    else:
        level_key = "advanced"
    
    if track in track_resources:
        resources = track_resources[track][level_key]
        for i, resource in enumerate(resources, 1):
            st.write(f"{i}. {resource}")
    
    # Export options
    st.subheader("ğŸ“¥ ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ“Š ØªØµØ¯ÙŠØ± ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„"):
            filename = env.export_session_data()
            st.success(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {filename}")
    
    with col2:
        if st.button("ğŸ¤– Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙˆÙƒÙŠÙ„"):
            filename = agent.save_model()
            st.success(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {filename}")
    
    with col3:
        if st.button("ğŸ“ˆ Ø¹Ø±Ø¶ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"):
            st.session_state.show_analytics = True
            st.rerun()
    
    # Restart option
    st.markdown("---")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±", type="primary"):
            # Reset session state
            for key in list(st.session_state.keys()):
                if key not in ['track_selector']:  # Keep track selection
                    del st.session_state[key]
            st.rerun()
    
    with col2:
        if st.button("ğŸ“Š Ø¹Ø±Ø¶ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª"):
            st.session_state.show_analytics = True
            st.rerun()

def main():
    """Main application function"""
    initialize_session_state()
    
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªÙƒÙŠÙÙŠ Ø§Ù„Ø°ÙƒÙŠ</h1>
        <p>ØªÙ‚ÙŠÙŠÙ… Ù‚Ø¯Ø±Ø§ØªÙƒ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø²</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    selected_track, agent_type, adaptation_strategy, max_questions, confidence_threshold = render_sidebar()
    
    # Update session state with sidebar values
    st.session_state.agent_type = agent_type
    st.session_state.adaptation_strategy = adaptation_strategy
    
    # Navigation
    if st.session_state.show_analytics:
        if st.button("â† Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"):
            st.session_state.show_analytics = False
            st.rerun()
        render_analytics()
        return
    
    # Main content based on state
    if not st.session_state.initialized:
        # Welcome screen
        st.header("ğŸš€ Ø§Ø¨Ø¯Ø£ Ø±Ø­Ù„Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ…")
        
        # Track statistics
        track_stats = get_question_statistics(selected_track)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader(f"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª {selected_track.upper()}")
            st.write(f"**Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:** {track_stats['total_questions']}")
            
            for level, info in track_stats['levels'].items():
                st.write(f"**{info['difficulty']}:** {info['count']} Ø³Ø¤Ø§Ù„")
        
        with col2:
            st.subheader("â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±")
            st.write(f"**Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø£Ø³Ø¦Ù„Ø©:** {max_questions}")
            st.write(f"**Ø¹ØªØ¨Ø© Ø§Ù„Ø«Ù‚Ø©:** {confidence_threshold:.0%}")
            st.write(f"**Ù†ÙˆØ¹ Ø§Ù„ÙˆÙƒÙŠÙ„:** {agent_type}")
            st.write(f"**Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªÙƒÙŠÙ:** {adaptation_strategy}")
        
        if st.button("ğŸ¯ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±", type="primary", use_container_width=True):
            # Initialize environment and agent
            st.session_state.env = AdaptiveAssessmentEnv(track=selected_track)
            st.session_state.env.max_questions = max_questions
            st.session_state.env.confidence_threshold = confidence_threshold
            
            # Initialize agent based on type
            if agent_type == "ensemble":
                st.session_state.agent = MultiAgentEnsemble(st.session_state.env)
            else:
                st.session_state.agent = RLAssessmentAgent(st.session_state.env)
            
            # Get first question
            first_question = st.session_state.env.get_question()
            if first_question:
                st.session_state.current_question = first_question
                st.session_state.initialized = True
                st.rerun()
            else:
                st.error("âŒ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ®ØµØµ.")
    
    elif st.session_state.show_results:
        render_results()
    
    else:
        # Main assessment interface
        if st.session_state.current_question:
            render_question()
        else:
            st.error("âŒ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„.")
    
    # Progress indicator
    if st.session_state.initialized and not st.session_state.show_results:
        env = st.session_state.env
        progress = min(env.total_questions_asked / env.max_questions, 1.0)
        
        st.sidebar.markdown("---")
        st.sidebar.subheader("ğŸ“ˆ Ø§Ù„ØªÙ‚Ø¯Ù…")
        st.sidebar.progress(progress)
        st.sidebar.write(f"Ø§Ù„Ø³Ø¤Ø§Ù„ {env.total_questions_asked} Ù…Ù† {env.max_questions}")
        
        # Real-time metrics
        if env.question_history:
            correct = sum(1 for q in env.question_history if q['is_correct'])
            accuracy = correct / len(env.question_history)
            
            st.sidebar.metric("Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©", f"{accuracy:.1%}")
            st.sidebar.metric("Ø§Ù„Ù‚Ø¯Ø±Ø© Ø§Ù„Ù…Ù‚Ø¯Ø±Ø©", f"{env.student_ability:.1%}")
            st.sidebar.metric("Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©", f"{env.confidence_score:.1%}")

if __name__ == "__main__":
    main()